{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AORjI8OMP1vn",
        "colab_type": "text"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQyvJf50Pw6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D , Flatten\n",
        "from keras.layers import TimeDistributed, LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from keras import backend as k\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyhnM6lKTWLW",
        "colab_type": "text"
      },
      "source": [
        "The convNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def video_generator(path , batch_size = 1 , train_data_size = 180 , test_data_size = 16 , mode = \"train\"):\n",
        "\n",
        "    count = 1\n",
        "    #infinite loop\n",
        "    if(mode == \"train\"):\n",
        "        while True:\n",
        "            #check if it is the end of train data size\n",
        "            #if its the end, set the counter to 1 for re-iteration\n",
        "            if(count > train_data_size):\n",
        "                count = 1\n",
        "                \n",
        "            #read hdf5\n",
        "            file = h5py.File(path + \"video (\" + str(count) + \").hdf5\" , \"r+\")\n",
        "            video = np.array(file[\"/video\"]).astype(\"uint8\").reshape((1 , 300 , 360 , 640 , 3))\n",
        "            label = np.array(file[\"/label\"]).astype(\"uint8\").reshape((1 , 1))\n",
        "\n",
        "            #label = np.log(label)\n",
        "            \n",
        "            #print(\"Now training on example \" + str(count))\n",
        "            count = count + 1\n",
        "            \n",
        "            #yield the input\n",
        "            yield (video , label)\n",
        "\n",
        "    elif(mode == \"eval\"):\n",
        "        while True:\n",
        "            #check if it is the end of train data size\n",
        "            #if its the end, set the counter to 1 for re-iteration\n",
        "            if(count > test_data_size):\n",
        "                count = 1\n",
        "                \n",
        "            #read hdf5\n",
        "            file = h5py.File(path + \"video (\" + str(count + train_data_size) + \").hdf5\" , \"r+\")\n",
        "            video = np.array(file[\"/video\"]).astype(\"uint8\").reshape((1 , 300 , 360 , 640 , 3))\n",
        "            label = np.array(file[\"/label\"]).astype(\"uint8\").reshape((1 , 1))\n",
        "\n",
        "            #label = np.log(label)\n",
        "\n",
        "            print(\"Now testing on example \" + str(count))\n",
        "\n",
        "            count = count + 1\n",
        "            \n",
        "            #yield the input\n",
        "            yield (video , label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aeAC7_5P4yR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_convnet(shape):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    #model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(32, (3,3), strides=(2 , 2) , input_shape=shape, padding='valid', activation='relu' , name = \"conv_1\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    #model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    #model.add(MaxPool2D())\n",
        "    \n",
        "    #model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(32, (3,3), strides=(2 , 2) , padding='valid', activation='relu' , name = \"conv_2\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    #model.add(Conv2D(1024, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(32, (1,1) , strides=(2 , 2) , padding='valid', activation='relu' , name = \"conv_3\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    # flatten...\n",
        "    #model.add(GlobalMaxPool2D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense((256) , activation=\"relu\"))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkq8xjdOTNGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model(shape):\n",
        "    # Create our convnet with (256 , 144 , 3) input shape\n",
        "    convnet = single_convnet(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with (200 , 256 , 144 , 3) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(LSTM(32))\n",
        "    # and finally, we make a decision network\n",
        "    #model.add(Dense(1024, activation='relu'))\n",
        "    #model.add(Dropout(.5))\n",
        "    #model.add(Dense(512, activation='relu'))\n",
        "    #model.add(Dropout(.5))\n",
        "    #model.add(Dense(128, activation='relu'))\n",
        "    #model.add(Dropout(.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(32 , activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJlpAxBVtXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cb18e81f-92cd-4016-ae49-8fde9d70d65b",
        "tags": []
      },
      "source": [
        "#load the already existing model along with weights etc.\n",
        "#model = keras.models.load_model(\"model\")\n",
        "\n",
        "#Else compile the model\n",
        "model = Model((300 , 360 , 640 , 3))\n",
        "model.summary()\n",
        "#load weights if there are saved weights, loads best weights\n",
        "#model.load_weights(\"model_1/variables/variables\")\n",
        "#load weights if there are saved weights, loads last weights\n",
        "model.load_weights(\"model_1\")\n",
        "model.compile(loss=keras.losses.MeanSquaredError(), optimizer='adam', metrics=['mse'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv_1 (Conv2D)              (None, 179, 319, 32)      896       \n_________________________________________________________________\ndropout (Dropout)            (None, 179, 319, 32)      0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 179, 319, 32)      128       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 89, 159, 32)       0         \n_________________________________________________________________\nconv_2 (Conv2D)              (None, 44, 79, 32)        9248      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 44, 79, 32)        0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 44, 79, 32)        128       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 22, 39, 32)        0         \n_________________________________________________________________\nconv_3 (Conv2D)              (None, 11, 20, 32)        1056      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 11, 20, 32)        0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 11, 20, 32)        128       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 5, 10, 32)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1600)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               409856    \n=================================================================\nTotal params: 421,440\nTrainable params: 421,248\nNon-trainable params: 192\n_________________________________________________________________\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntime_distributed (TimeDistri (None, 300, 256)          421440    \n_________________________________________________________________\nlstm (LSTM)                  (None, 32)                36992     \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 460,577\nTrainable params: 460,385\nNon-trainable params: 192\n_________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQe2YbQ6mS52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "601be522-34c7-4519-ec1d-a22a4f087eb1",
        "tags": []
      },
      "source": [
        "#this cell is for testing the model with mock data only\n",
        "#do not use in the actual implementation\n",
        "x = np.ones((1 , 300 , 360 , 640 , 3))\n",
        "y = np.array([.25])\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    model.fit(x , y , epochs = 50 , callbacks=[tensorboard_callback])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/50\n1/1 [==============================] - 0s 454ms/step - loss: 0.0081 - mse: 0.0081\nEpoch 2/50\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTm4rz_mp57o",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "#This cell is for training\n",
        "#setting up tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#setting up checkpoint\n",
        "checkpoint = ModelCheckpoint(\"model_1\", monitor='loss', verbose=1, save_best_only=True, mode='auto', save_freq=20)\n",
        "\n",
        "#setting the path for training and test data\n",
        "path = \"YoutubeScrapper/python/out/compresseddata/\"\n",
        "\n",
        "#Assigning batch size\n",
        "batch_size = 1\n",
        "\n",
        "#Assigning training data size\n",
        "train_data_size = 180\n",
        "\n",
        "#Assigning test data size\n",
        "test_data_size = 16\n",
        "\n",
        "#Initializing the generator\n",
        "training_data = video_generator(path , batch_size=1 , train_data_size = train_data_size , test_data_size = test_data_size , mode = \"train\")\n",
        "\n",
        "\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    model.fit(x=training_data , steps_per_epoch=train_data_size , epochs=1 , callbacks=[tensorboard_callback , checkpoint])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "19/180 [==>...........................] - ETA: 1:32:06 - loss: 183.6883 - mse: 183.6883\nEpoch 00001: loss improved from inf to 174.71086, saving model to model_1\nINFO:tensorflow:Assets written to: model_1\\assets\n 39/180 [=====>........................] - ETA: 1:34:04 - loss: 229.3042 - mse: 229.3042\nEpoch 00001: loss did not improve from 174.71086\n 59/180 [========>.....................] - ETA: 1:19:34 - loss: 224.3844 - mse: 224.3844\nEpoch 00001: loss did not improve from 174.71086\n 79/180 [============>.................] - ETA: 1:06:08 - loss: 204.4345 - mse: 204.4345\nEpoch 00001: loss did not improve from 174.71086\n 99/180 [===============>..............] - ETA: 55:22 - loss: 206.8901 - mse: 206.8901\nEpoch 00001: loss did not improve from 174.71086\n119/180 [==================>...........] - ETA: 41:58 - loss: 204.5099 - mse: 204.5099\nEpoch 00001: loss did not improve from 174.71086\n139/180 [======================>.......] - ETA: 29:24 - loss: 188.7279 - mse: 188.7279\nEpoch 00001: loss did not improve from 174.71086\n159/180 [=========================>....] - ETA: 14:44 - loss: 190.5746 - mse: 190.5746\nEpoch 00001: loss did not improve from 174.71086\n179/180 [============================>.] - ETA: 42s - loss: 188.3069 - mse: 188.3069\nEpoch 00001: loss did not improve from 174.71086\n180/180 [==============================] - 7650s 42s/step - loss: 187.7356 - mse: 187.7356\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_weights(\"model_1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Now testing on example 1\nNow testing on example 2\n 1/16 [>.............................] - ETA: 0s - loss: 0.0149 - mse: 0.0149Now testing on example 3\n 2/16 [==>...........................] - ETA: 16s - loss: 0.0123 - mse: 0.0123Now testing on example 4\n 3/16 [====>.........................] - ETA: 20s - loss: 0.0086 - mse: 0.0086Now testing on example 5\n 4/16 [======>.......................] - ETA: 20s - loss: 0.0064 - mse: 0.0064Now testing on example 6\n 5/16 [========>.....................] - ETA: 21s - loss: 0.0056 - mse: 0.0056Now testing on example 7\n 6/16 [==========>...................] - ETA: 21s - loss: 0.0056 - mse: 0.0056Now testing on example 8\n 7/16 [============>.................] - ETA: 20s - loss: 0.0048 - mse: 0.0048Now testing on example 9\n 8/16 [==============>...............] - ETA: 19s - loss: 0.0563 - mse: 0.0563Now testing on example 10\n 9/16 [===============>..............] - ETA: 17s - loss: 0.0507 - mse: 0.0507Now testing on example 11\n10/16 [=================>............] - ETA: 15s - loss: 0.0686 - mse: 0.0686Now testing on example 12\n11/16 [===================>..........] - ETA: 12s - loss: 0.0634 - mse: 0.0634Now testing on example 13\n12/16 [=====================>........] - ETA: 10s - loss: 0.0623 - mse: 0.0623Now testing on example 14\n13/16 [=======================>......] - ETA: 7s - loss: 0.0653 - mse: 0.0653Now testing on example 15\n14/16 [=========================>....] - ETA: 5s - loss: 0.0653 - mse: 0.0653Now testing on example 16\n15/16 [===========================>..] - ETA: 2s - loss: 0.0628 - mse: 0.0628Now testing on example 1\n16/16 [==============================] - 42s 3s/step - loss: 0.0859 - mse: 0.0859\n"
        }
      ],
      "source": [
        "#This cell is for testing the model against test data\n",
        "#Assigning batch size\n",
        "batch_size = 1\n",
        "\n",
        "#Assigning training data size\n",
        "train_data_size = 180\n",
        "\n",
        "#setting the path for training and test data\n",
        "path = \"YoutubeScrapper/python/out/compresseddata/\"\n",
        "\n",
        "#Assigning test data size\n",
        "test_data_size = 16\n",
        "\n",
        "#evaluating data\n",
        "test_data = video_generator(path , mode = \"eval\")\n",
        "\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    model.evaluate(test_data , steps = test_data_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[2.6431156  3.21012187]\n [3.33283979 3.21012187]\n [3.09572022 3.1907053 ]\n [3.20269007 3.21012187]\n [3.2380752  3.2100563 ]\n [3.29547809 3.21012187]\n [3.17756094 3.21012187]\n [3.24399498 3.21012187]\n [2.58748552 3.21012187]\n [3.14972174 3.21012187]\n [3.69619529 3.21012187]\n [2.91853983 2.99736738]\n [3.43680669 3.21012187]\n [2.93598227 3.21012187]\n [3.48510001 3.21012187]\n [3.08298207 3.21012187]]\n"
        }
      ],
      "source": [
        "results = np.zeros((16 , 2))\n",
        "\n",
        "for i in range(16):\n",
        "    with h5py.File(path + \"video (\" + str(180 + i) + \").hdf5\" , \"r\") as rfile:\n",
        "        results[i , 0] = np.log(rfile[(\"label\")].value)\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            results[i , 1] = model.predict(rfile[\"video\"].value.reshape((1 , 300 , 360 , 640 , 3)))\n",
        "\n",
        "print(results)\n",
        "mse = np.mean(np.square(results[: , 0] - results[: , 1]))\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[28.35748792 16.7230444  38.95047857 15.08353609 29.05002875 18.12231653\n   2.01449275 11.69917479 20.75453048 40.63234323 26.0279537  17.2093242\n  20.40818691 44.45222622 49.31662488 41.24226234 29.86590352 49.92768595\n  23.98516749 22.99521531 14.12142039 26.95626517 11.49900596 18.4933084\n  23.77627628 20.06585062 46.0832495  41.57132353 78.66545455 11.35929648\n  17.39303483 18.29154973 21.25208467 37.77776466 52.18252212 31.42992271\n  18.33649635 10.57059039 36.41602465 27.77971125 19.53349876 21.5\n  24.51872247 20.66812074 54.07916806 64.03370325  4.73398548 23.92034915\n  21.25563063 28.03225806 31.35063704 33.17158253 21.82569891  7.82915832\n  11.65238034 56.40984268 30.77326671 30.77326671 11.09141147 11.36036036\n  25.8745098  12.11494253 20.63751584 37.60014728 28.93274913 26.23485259\n  19.024      12.28061423 20.31459436 22.94752775  3.78790229 22.75\n  39.96127563 26.37772277 55.49603461  9.14388489 31.1127008  21.01385042\n  21.71345595 58.33333333 37.84904259 15.59815774 18.71746134 13.2972973\n  13.52824427 49.03302852 35.55136594 13.82688928 19.48509025 15.26416289\n  31.65957447 10.87987988 21.77894737 18.83568075 61.2012848  25.43995098\n  18.70130994 21.04631508 25.55728235 47.93596059  4.99303123 17.109375\n  22.62737643 40.28056011 31.22641509 26.01893939 20.35185185  6.78881207\n  11.69781484 42.26034063 17.87926509 25.05291005 23.20361407  7.34181055\n  57.81218274 20.63517681 10.61351999 33.79180328 19.53727811  6.78225144\n  19.07191849 22.96470588 28.16393443 17.68235294 14.43478261 25.56656721\n  21.14508929 13.5146291  24.67132867 26.32373314 13.81578947 14.87381546\n  31.29084967 52.32276171 18.50874111 20.64769617 20.79617834 29.70718232\n   5.56722134 30.12254142 17.8144911  14.01282051 31.08896721  3.54014599\n  42.01493264 50.26086957  5.19977316  9.59574468 11.17322835 17.5948856\n  17.01157831 14.31227306 16.75065847 15.34508197 31.16774922 60.80105973\n  25.03015075 16.49358974 30.95019157 12.54757515  7.85549593 39.31102613\n  33.03422053 35.1296698  33.83463687 24.54017857 13.84663866 15.04285714\n  16.67304493 50.61504261 21.60633484 55.00584795 42.9673913  34.19269269\n  27.06596306  8.4031941  29.36956522 17.76649746 14.14285714 14.05693127]]\n25.247515105643686\n21.65989539758118\n131\n"
        }
      ],
      "source": [
        "values = np.zeros((1 , 180))\n",
        "for i in range(180):\n",
        "    with h5py.File(path + \"video (\" + str(i + 1) + \").hdf5\" , \"r\") as rfile:\n",
        "        values[0 , i] = np.log(rfile[(\"label\")].value)\n",
        "\n",
        "print(values)\n",
        "print(np.mean(values))\n",
        "data_mean = np.mean(values)\n",
        "print(np.median(values))\n",
        "\n",
        "mse = np.zeros((1 , 230))\n",
        "for i in range(230):\n",
        "    mse[0 , i] = np.mean(np.square(results[: , 0] - (3.07 + ((i + 1) / 1000))))\n",
        "    \n",
        "\n",
        "print(np.argmin(mse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}